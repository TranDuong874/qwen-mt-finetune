# Experiment
experiment_name: "qwen-cpt-medical-mt"

# Model
base_model: "Qwen/Qwen2.5-3B"
adapter_path: null  # Set to path to resume from existing adapter

# LoRA Configuration
lora:
  r: 16
  alpha: 32
  dropout: 0.0
  target_modules:
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "o_proj"
    - "gate_proj"
    - "up_proj"
    - "down_proj"

# Quantization (QLoRA) - disabled for multi-GPU DDP
# Set load_in_4bit: true for single-GPU with limited VRAM
quantization:
  load_in_4bit: false
  bnb_4bit_quant_type: "nf4"
  bnb_4bit_compute_dtype: "bfloat16"
  bnb_4bit_use_double_quant: true

# Dataset (streaming from HuggingFace)
dataset:
  hf_repo: "TranDuong/medical-vlsp-2025"

# Training
training:
  num_epochs: 1
  per_device_batch_size: 8
  gradient_accumulation_steps: 4
  learning_rate: 2.0e-4
  warmup_ratio: 0.03
  max_seq_length: 512
  fp16: false
  bf16: true
  gradient_checkpointing: true
  scheduler: "cosine"
  dataloader_num_workers: 4  # Speed up data loading
  # Logging & Evaluation
  logging_steps: 50
  eval_steps: 500
  save_steps: 500
  save_total_limit: 2

# Evaluation (for evaluate.py)
evaluation:
  max_new_tokens: 256
  generation_batch_size: 32
  comet_model: "Unbabel/wmt22-comet-da"
  comet_batch_size: 32
  log_examples: true
  num_examples_to_log: 100

# W&B
wandb:
  project: "qwen-mt-cpt"
  enabled: true

# HuggingFace Hub
huggingface:
  push_best_model: true
  repo_id: "TranDuong/qwen-medical-mt-cpt"
  private: true

# Output
output_dir: "outputs"
