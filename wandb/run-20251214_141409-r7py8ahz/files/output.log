Generating train split: 100000 examples [00:00, 152111.10 examples/s]
Generating valid split: 20002 examples [00:00, 261659.16 examples/s]
Loading checkpoint shards: 100%|█| 2/2 [00:06<00:00,  3.48s/it
Loading adapter from outputs/smoke_test-part1/best_model
trainable params: 0 || all params: 1,738,007,552 || trainable%: 0.0000
Map: 100%|████████████| 10/10 [00:00<00:00, 387.52 examples/s]
Map: 100%|███████████| 10/10 [00:00<00:00, 1659.93 examples/s]
  0%|                                  | 0/10 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Traceback (most recent call last):
  File "/home/tranduong/qwen-mt-finetune/train.py", line 154, in <module>
    train(
  File "/home/tranduong/qwen-mt-finetune/train.py", line 130, in train
    trainer.train()
  File "/home/tranduong/qwen-mt-finetune/.venv/lib/python3.11/site-packages/transformers/trainer.py", line 2325, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/tranduong/qwen-mt-finetune/.venv/lib/python3.11/site-packages/transformers/trainer.py", line 2740, in _inner_training_loop
    self.optimizer.step()
  File "/home/tranduong/qwen-mt-finetune/.venv/lib/python3.11/site-packages/accelerate/optimizer.py", line 167, in step
    self.scaler.update()
  File "/home/tranduong/qwen-mt-finetune/.venv/lib/python3.11/site-packages/torch/amp/grad_scaler.py", line 521, in update
    assert len(found_infs) > 0, "No inf checks were recorded prior to update."
           ^^^^^^^^^^^^^^^^^^^
AssertionError: No inf checks were recorded prior to update.
